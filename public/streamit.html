<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>StreamIt â€“ Live DEâ†’EN</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; padding: 16px; }
    button { font-size: 18px; padding: 8px 16px; }
    #log { white-space: pre-wrap; margin-top: 12px; font-family: ui-monospace, Menlo, monospace; }
  </style>
</head>
<body>
  <h1>StreamIt â€“ Live (DE â†’ EN)</h1>
  <button id="start">START</button>
  <button id="stop" disabled>STOP</button>
  <div id="log"></div>

<script>
const log = (...a) => { document.getElementById('log').textContent += a.join(' ') + '\n'; };

let pc, micStream, dataChannel, subsWS;

async function getEphemeral() {
  const r = await fetch('/session', { method: 'POST' });
  if (!r.ok) throw new Error('Session failed');
  return r.json();
}

async function start() {
  try {
    document.getElementById('start').disabled = true;

    // 1) Mic vom iPad holen
    log('Fordere Mikrofon-Zugriff an...');
    micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    log('âœ“ Mic OK');

    // 2) Ephemeral Token + Realtime WebRTC
    log('Hole Session-Token...');
    const sess = await getEphemeral();
    log('Session-Antwort:', JSON.stringify(sess, null, 2));
    
    const EPHEMERAL_KEY = sess.client_secret?.value || sess.client_secret || sess?.client_secret_key;
    if (!EPHEMERAL_KEY) {
      throw new Error('Kein Ephemeral Key erhalten! Session: ' + JSON.stringify(sess));
    }
    log('âœ“ Ephemeral Key erhalten');
    
    const MODEL_URL = "https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview-2024-10-01";

    log('Erstelle WebRTC-Verbindung...');
    pc = new RTCPeerConnection();
    micStream.getTracks().forEach(t => pc.addTrack(t, micStream));

    // DataChannel, um Realtime-Events als JSON zu erhalten
    dataChannel = pc.createDataChannel("oai-events");
    dataChannel.onopen = () => log('âœ“ DataChannel offen');
    dataChannel.onerror = (e) => log('âŒ DataChannel Fehler:', e);
    dataChannel.onclose = () => log('DataChannel geschlossen');
    
    dataChannel.onmessage = (e) => {
      try {
        const ev = JSON.parse(e.data);
        
        // Nur wichtige Events detailliert loggen
        if (ev.type.includes('transcript') || ev.type.includes('text')) {
          log('ðŸ“ Event:', ev.type, JSON.stringify(ev, null, 2));
        } else {
          log('Event:', ev.type);
        }
        
        // Verschiedene Event-Typen die OpenAI senden kann:
        
        // 1. Audio-Transkript Delta (Echtzeit-Ãœbersetzung)
        if (ev.type === 'response.audio_transcript.delta' && ev.delta) {
          log('â†’ Sende partial:', ev.delta);
          publishSub({ type: 'partial', text: ev.delta });
        }
        
        // 2. Audio-Transkript Done (VollstÃ¤ndige Ãœbersetzung)
        if (ev.type === 'response.audio_transcript.done' && ev.transcript) {
          log('â†’ Sende final:', ev.transcript);
          publishSub({ type: 'final', text: ev.transcript });
        }
        
        // 3. Eingabe-Transkription (was der User gesagt hat)
        if (ev.type === 'conversation.item.input_audio_transcription.completed' && ev.transcript) {
          log('â†’ Input transkribiert:', ev.transcript);
          publishSub({ type: 'partial', text: '[DE] ' + ev.transcript });
        }
        
        if (ev.type === 'conversation.item.input_audio_transcription.delta' && ev.delta) {
          log('â†’ Input delta:', ev.delta);
        }
        
        // 4. Text-basierte Responses (falls Modell Text zurÃ¼ckgibt)
        if (ev.type === 'response.output_text.delta' && ev.delta) {
          log('â†’ Sende text delta:', ev.delta);
          publishSub({ type: 'partial', text: ev.delta });
        }
        
        if (ev.type === 'response.output_text.done' && ev.text) {
          log('â†’ Sende text final:', ev.text);
          publishSub({ type: 'final', text: ev.text });
        }
        
        if (ev.type === 'response.text.delta' && ev.delta) {
          log('â†’ Sende text delta:', ev.delta);
          publishSub({ type: 'partial', text: ev.delta });
        }
        
        if (ev.type === 'response.text.done' && ev.text) {
          log('â†’ Sende text final:', ev.text);
          publishSub({ type: 'final', text: ev.text });
        }
        
      } catch (err) {
        log('âŒ Fehler beim Event-Parsing:', err);
      }
    };

    // Remote audio (falls Modell sprechen wÃ¼rde â€“ bei uns deaktiviert)
    pc.ontrack = (e) => {
      log('Remote track erhalten (nicht verwendet)');
    };

    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);
    log('âœ“ Offer erstellt');

    log('Sende SDP an OpenAI...');
    const sdpResp = await fetch(MODEL_URL, {
      method: "POST",
      body: offer.sdp,
      headers: {
        Authorization: `Bearer ${EPHEMERAL_KEY}`,
        "Content-Type": "application/sdp"
      }
    });
    
    if (!sdpResp.ok) {
      const errorText = await sdpResp.text();
      throw new Error(`OpenAI SDP Fehler (${sdpResp.status}): ${errorText}`);
    }
    
    const answer = { type: "answer", sdp: await sdpResp.text() };
    await pc.setRemoteDescription(answer);
    log('âœ“ WebRTC-Verbindung etabliert');

    // 3) WS zu unserem Server fÃ¼r Subtitle-Broadcast
    log('Verbinde zu Subtitle-WebSocket...');
    subsWS = new WebSocket((location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws/subs');
    subsWS.onopen = () => log('âœ“ Subs WS verbunden');
    subsWS.onerror = (e) => log('âŒ Subs WS Fehler:', e);
    subsWS.onclose = () => log('Subs WS geschlossen');

    document.getElementById('stop').disabled = false;
    log('ðŸŽ‰ Alles bereit! Sprich ins Mikrofon...');
    
  } catch (err) {
    log('âŒ FEHLER beim Start:', err.message);
    log('Stack:', err.stack);
    document.getElementById('start').disabled = false;
    // Cleanup bei Fehler
    try { micStream && micStream.getTracks().forEach(t => t.stop()); } catch {}
    try { pc && pc.close(); } catch {}
  }
}

function publishSub(payload) {
  // payload: {type: "partial"|"final", text: "..."}
  if (subsWS && subsWS.readyState === 1 && payload.text && payload.text.trim()) {
    subsWS.send(JSON.stringify(payload));
  }
}

async function stop() {
  document.getElementById('stop').disabled = true;
  try { dataChannel && dataChannel.close(); } catch {}
  try { pc && pc.close(); } catch {}
  try { micStream && micStream.getTracks().forEach(t => t.stop()); } catch {}
  try { subsWS && subsWS.close(); } catch {}
  document.getElementById('start').disabled = false;
  log('Stopped.');
}

document.getElementById('start').onclick = start;
document.getElementById('stop').onclick = stop;
</script>
</body>
</html>
