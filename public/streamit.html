<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>StreamIt – Live DE→EN</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; padding: 16px; }
    button { font-size: 18px; padding: 8px 16px; }
    #log { white-space: pre-wrap; margin-top: 12px; font-family: ui-monospace, Menlo, monospace; }
  </style>
</head>
<body>
  <h1>StreamIt – Live (DE → EN)</h1>
  <button id="start">START</button>
  <button id="stop" disabled>STOP</button>
  <div id="log"></div>

<script>
const log = (...a) => { document.getElementById('log').textContent += a.join(' ') + '\n'; };

let pc, micStream, dataChannel, subsWS;

async function getEphemeral() {
  const r = await fetch('/session', { method: 'POST' });
  if (!r.ok) throw new Error('Session failed');
  return r.json();
}

async function start() {
  document.getElementById('start').disabled = true;

  // 1) Mic vom iPad holen
  micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  log('Mic OK');

  // 2) Ephemeral Token + Realtime WebRTC
  const sess = await getEphemeral();
  const EPHEMERAL_KEY = sess.client_secret?.value || sess.client_secret || sess?.client_secret_key;
  const MODEL_URL = "https://api.openai.com/v1/realtime?model=gpt-4o-realtime-preview";

  pc = new RTCPeerConnection();
  micStream.getTracks().forEach(t => pc.addTrack(t, micStream));

  // DataChannel, um Realtime-Events als JSON zu erhalten
  dataChannel = pc.createDataChannel("oai-events");
  dataChannel.onmessage = (e) => {
    try {
      const ev = JSON.parse(e.data);
      // Wichtige Events: response.output_text.delta / response.output_text.done / transcript.delta (je nach Modell)
      if (ev.type === 'response.output_text.delta' && ev.delta) {
        publishSub({ type: 'partial', text: ev.delta });
      }
      if (ev.type === 'response.output_text.done' && ev.text) {
        publishSub({ type: 'final', text: ev.text });
      }
      // Manche Varianten senden 'response.audio_transcript.delta' etc.
      if (ev.type?.includes('transcript') && ev.delta) {
        publishSub({ type: 'partial', text: ev.delta });
      }
    } catch {}
  };

  // Remote audio (falls Modell sprechen würde – bei uns deaktiviert)
  pc.ontrack = (e) => {
    // not used (no TTS requested)
  };

  const offer = await pc.createOffer();
  await pc.setLocalDescription(offer);

  const sdpResp = await fetch(MODEL_URL, {
    method: "POST",
    body: offer.sdp,
    headers: {
      Authorization: `Bearer ${EPHEMERAL_KEY}`,
      "Content-Type": "application/sdp"
    }
  });
  const answer = { type: "answer", sdp: await sdpResp.text() };
  await pc.setRemoteDescription(answer);

  // 3) WS zu unserem Server für Subtitle-Broadcast
  subsWS = new WebSocket((location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws/subs');
  subsWS.onopen = () => log('Subs WS connected');
  subsWS.onerror = (e) => log('Subs WS error', e);
  subsWS.onclose = () => log('Subs WS closed');

  document.getElementById('stop').disabled = false;
  log('Realtime connected');
}

function publishSub(payload) {
  // payload: {type: "partial"|"final", text: "..."}
  if (subsWS && subsWS.readyState === 1 && payload.text && payload.text.trim()) {
    subsWS.send(JSON.stringify(payload));
  }
}

async function stop() {
  document.getElementById('stop').disabled = true;
  try { dataChannel && dataChannel.close(); } catch {}
  try { pc && pc.close(); } catch {}
  try { micStream && micStream.getTracks().forEach(t => t.stop()); } catch {}
  try { subsWS && subsWS.close(); } catch {}
  document.getElementById('start').disabled = false;
  log('Stopped.');
}

document.getElementById('start').onclick = start;
document.getElementById('stop').onclick = stop;
</script>
</body>
</html>
