<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>StreamIt ‚Äì Live DE‚ÜíEN</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    body { font-family: system-ui, sans-serif; padding: 16px; background: #1a1a1a; color: #fff; }
    button { font-size: 18px; padding: 12px 24px; margin-right: 8px; border-radius: 6px; border: none; cursor: pointer; }
    #start { background: #10a37f; color: white; }
    #start:disabled { background: #666; cursor: not-allowed; }
    #stop { background: #ef4444; color: white; }
    #stop:disabled { background: #666; cursor: not-allowed; }
    .status { margin: 20px 0; padding: 12px; background: #2a2a2a; border-radius: 6px; }
    .status.recording { background: #10a37f22; border: 2px solid #10a37f; }
    #log { white-space: pre-wrap; margin-top: 12px; font-family: ui-monospace, Menlo, monospace; font-size: 13px; line-height: 1.6; }
  </style>
</head>
<body>
  <h1>üéôÔ∏è StreamIt ‚Äì Live DE ‚Üí EN Translation</h1>
  <div>
    <button id="start">START RECORDING</button>
    <button id="stop" disabled>STOP</button>
  </div>
  <div class="status" id="status">Bereit</div>
  <div id="log"></div>

<script>
const log = (...a) => { 
  const msg = a.join(' ');
  console.log(...a);
  document.getElementById('log').textContent += msg + '\n'; 
};

let mediaRecorder = null;
let audioChunks = [];
let recordingInterval = null;
let subsWS = null;
let audioContext = null;
let analyser = null;
let hasRecentSpeech = false;

async function start() {
  try {
    document.getElementById('start').disabled = true;
    document.getElementById('status').textContent = 'Starte...';
    
    // 1) Mikrofon-Zugriff
    log('üé§ Fordere Mikrofon-Zugriff an...');
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    log('‚úÖ Mikrofon-Zugriff erhalten');
    
    // Audio-Analyse f√ºr Voice Activity Detection
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    analyser = audioContext.createAnalyser();
    const source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);
    analyser.fftSize = 2048;
    
    // Starte Audio-Monitoring
    monitorAudioLevel();
    log('‚úÖ Voice Activity Detection aktiv');
    
    // 2) WebSocket zu Server f√ºr Subtitle-Broadcast
    subsWS = new WebSocket((location.protocol === 'https:' ? 'wss://' : 'ws://') + location.host + '/ws/subs');
    subsWS.onopen = () => log('‚úÖ WebSocket verbunden');
    subsWS.onerror = (e) => log('‚ùå WebSocket Fehler:', e);
    subsWS.onclose = () => log('‚ö†Ô∏è WebSocket geschlossen');
    
    // 3) MediaRecorder Setup
    // Nutze webm/opus f√ºr beste Kompatibilit√§t
    const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
      ? 'audio/webm;codecs=opus' 
      : 'audio/webm';
    
    mediaRecorder = new MediaRecorder(stream, { mimeType });
    log('‚úÖ MediaRecorder erstellt:', mimeType);
    
    audioChunks = [];
    
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.push(event.data);
        log(`üì¶ Audio-Chunk empfangen: ${(event.data.size / 1024).toFixed(1)} KB`);
      }
    };
    
    mediaRecorder.onstop = async () => {
      if (audioChunks.length > 0) {
        await sendAudioForTranscription();
      }
    };
    
    // 4) Starte Aufnahme
    mediaRecorder.start();
    log('üî¥ Aufnahme gestartet');
    
    // 5) Alle 2 Sekunden: Stop ‚Üí Send ‚Üí Restart (nur wenn Sprache erkannt)
    // K√ºrzere Intervalle = schnellere Live-Reaktion
    recordingInterval = setInterval(() => {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        if (hasRecentSpeech) {
          mediaRecorder.stop();
          hasRecentSpeech = false; // Reset f√ºr n√§chsten Chunk
          
          // Starte neue Aufnahme sofort
          setTimeout(() => {
            if (mediaRecorder) {
              audioChunks = [];
              mediaRecorder.start();
            }
          }, 50);
        } else {
          audioChunks = []; // Leere Chunks verwerfen
        }
      }
    }, 2000); // 2 Sekunden statt 4 = doppelt so schnell!
    
    document.getElementById('stop').disabled = false;
    document.getElementById('status').textContent = 'üî¥ AUFNAHME L√ÑUFT';
    document.getElementById('status').classList.add('recording');
    log('üéâ Bereit! Sprich auf Deutsch...');
    
  } catch (err) {
    log('‚ùå FEHLER:', err.message);
    document.getElementById('start').disabled = false;
    document.getElementById('status').textContent = 'Fehler: ' + err.message;
  }
}

// Voice Activity Detection - pr√ºft ob aktuell gesprochen wird
function monitorAudioLevel() {
  if (!analyser) return;
  
  const dataArray = new Uint8Array(analyser.frequencyBinCount);
  
  const checkLevel = () => {
    if (!analyser) return;
    
    analyser.getByteTimeDomainData(dataArray);
    
    // Berechne RMS (Root Mean Square) f√ºr Audio-Energie
    let sum = 0;
    for (let i = 0; i < dataArray.length; i++) {
      const normalized = (dataArray[i] - 128) / 128;
      sum += normalized * normalized;
    }
    const rms = Math.sqrt(sum / dataArray.length);
    
    // Schwellwert f√ºr Sprache (experimentell anpassbar)
    const SPEECH_THRESHOLD = 0.02;
    
    if (rms > SPEECH_THRESHOLD) {
      hasRecentSpeech = true;
    }
    
    requestAnimationFrame(checkLevel);
  };
  
  checkLevel();
}

async function sendAudioForTranscription() {
  try {
    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
    const startTime = Date.now();
    
    const formData = new FormData();
    formData.append('audio', audioBlob, 'audio.webm');
    
    const response = await fetch('/transcribe', {
      method: 'POST',
      body: formData
    });
    
    if (!response.ok) {
      const errorText = await response.text();
      throw new Error(`Server-Fehler (${response.status}): ${errorText}`);
    }
    
    const result = await response.json();
    const duration = ((Date.now() - startTime) / 1000).toFixed(1);
    
    if (result.filtered) {
      log(`üö´ Halluzination gefiltert`);
      return;
    }
    
    if (result.english) {
      log(`‚ö° ${duration}s ‚Üí üá¨üáß "${result.english}"`);
    }
    
  } catch (err) {
    log('‚ùå Fehler:', err.message);
  }
}

function stop() {
  document.getElementById('stop').disabled = true;
  document.getElementById('status').classList.remove('recording');
  
  if (recordingInterval) {
    clearInterval(recordingInterval);
    recordingInterval = null;
  }
  
  if (mediaRecorder && mediaRecorder.state !== 'inactive') {
    mediaRecorder.stop();
    mediaRecorder.stream.getTracks().forEach(track => track.stop());
  }
  
  if (audioContext) {
    audioContext.close();
    audioContext = null;
    analyser = null;
  }
  
  if (subsWS) {
    subsWS.close();
    subsWS = null;
  }
  
  mediaRecorder = null;
  audioChunks = [];
  hasRecentSpeech = false;
  
  document.getElementById('start').disabled = false;
  document.getElementById('status').textContent = 'Gestoppt';
  log('‚èπÔ∏è Aufnahme beendet');
}

document.getElementById('start').onclick = start;
document.getElementById('stop').onclick = stop;
</script>
</body>
</html>
